{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the trubrics-sdk Trubrics enables AI teams to collect, analyse and manage user prompts & feedback on models. This allows teams to: \ud83d\udea8 Identify bugs - users are constantly running inference on models, and may be more likely to find bugs than an ML monitoring system \ud83e\uddd1\u200d\ud83d\udcbb\ufe0f Fine tune - users often hold domain knowledge that can be useful to fine tune models \ud83d\udc65 Align - identifying user preferences will help to align models to users Try our LLM demo Create your free account : Save feedback to Trubrics from our demo LLM app : Or watch a step by step video of integrating Trubrics into the LLM Streamlit app here . Collect user prompts & feedback with the Python SDK The python SDK allows you to collect user prompts & feedback from your ML apps from any python backend or web framework. Install it with: pip install trubrics Now set your Trubrics email and password as environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" and push some user prompts & feedback to the default project & feedback component: import os from trubrics import Trubrics trubrics = Trubrics ( project = \"default\" , email = os . environ [ \"TRUBRICS_EMAIL\" ], password = os . environ [ \"TRUBRICS_PASSWORD\" ], ) user_prompt = trubrics . log_prompt ( config_model = { \"model\" : \"gpt-3.5-turbo\" }, prompt = \"Tell me a joke\" , generation = \"Why did the chicken cross the road? To get to the other side.\" , ) user_feedback = trubrics . log_feedback ( component = \"default\" , model = user_prompt . config_model . model , prompt_id = user_prompt . id , user_response = { \"type\" : \"thumbs\" , \"score\" : \"\ud83d\udc4e\" , \"text\" : \"Not a very funny joke...\" , } ) Collect user prompts & feedback from a Streamlit app To start collecting user feedback from your Streamlit app, install the additional dependency: pip install \"trubrics[streamlit]\" and test this code snippet in your app: import streamlit as st from trubrics.integrations.streamlit import FeedbackCollector collector = FeedbackCollector ( email = st . secrets . TRUBRICS_EMAIL , password = st . secrets . TRUBRICS_PASSWORD , project = \"default\" ) user_feedback = collector . st_feedback ( component = \"default\" , feedback_type = \"thumbs\" , open_feedback_label = \"[Optional] Provide additional feedback\" , model = \"gpt-3.5-turbo\" , prompt_id = None , # checkout collector.log_prompt() to log your user prompts ) if user_feedback : st . write ( \"#### Raw feedback saved to Trubrics:\" ) st . write ( user_feedback ) For a full examples logging user prompts and feedback in Streamlit, see our Streamlit integration docs . Collect user feedback from a React.js app To collect user feedback from a React application, check out this example . What's next? If you haven't already, create a free account or sign in to Trubrics . Get more technical information from our docs : Collect & analyse user prompts Collect & analyse user feedback Manage user feedback with Issues Check out our website for more information about Trubrics.","title":"Getting Started"},{"location":"#welcome-to-the-trubrics-sdk","text":"Trubrics enables AI teams to collect, analyse and manage user prompts & feedback on models. This allows teams to: \ud83d\udea8 Identify bugs - users are constantly running inference on models, and may be more likely to find bugs than an ML monitoring system \ud83e\uddd1\u200d\ud83d\udcbb\ufe0f Fine tune - users often hold domain knowledge that can be useful to fine tune models \ud83d\udc65 Align - identifying user preferences will help to align models to users","title":"Welcome to the trubrics-sdk"},{"location":"#try-our-llm-demo","text":"Create your free account : Save feedback to Trubrics from our demo LLM app : Or watch a step by step video of integrating Trubrics into the LLM Streamlit app here .","title":"Try our LLM demo"},{"location":"#collect-user-prompts-feedback-with-the-python-sdk","text":"The python SDK allows you to collect user prompts & feedback from your ML apps from any python backend or web framework. Install it with: pip install trubrics Now set your Trubrics email and password as environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" and push some user prompts & feedback to the default project & feedback component: import os from trubrics import Trubrics trubrics = Trubrics ( project = \"default\" , email = os . environ [ \"TRUBRICS_EMAIL\" ], password = os . environ [ \"TRUBRICS_PASSWORD\" ], ) user_prompt = trubrics . log_prompt ( config_model = { \"model\" : \"gpt-3.5-turbo\" }, prompt = \"Tell me a joke\" , generation = \"Why did the chicken cross the road? To get to the other side.\" , ) user_feedback = trubrics . log_feedback ( component = \"default\" , model = user_prompt . config_model . model , prompt_id = user_prompt . id , user_response = { \"type\" : \"thumbs\" , \"score\" : \"\ud83d\udc4e\" , \"text\" : \"Not a very funny joke...\" , } )","title":"Collect user prompts &amp; feedback with the Python SDK"},{"location":"#collect-user-prompts-feedback-from-a-streamlit-app","text":"To start collecting user feedback from your Streamlit app, install the additional dependency: pip install \"trubrics[streamlit]\" and test this code snippet in your app: import streamlit as st from trubrics.integrations.streamlit import FeedbackCollector collector = FeedbackCollector ( email = st . secrets . TRUBRICS_EMAIL , password = st . secrets . TRUBRICS_PASSWORD , project = \"default\" ) user_feedback = collector . st_feedback ( component = \"default\" , feedback_type = \"thumbs\" , open_feedback_label = \"[Optional] Provide additional feedback\" , model = \"gpt-3.5-turbo\" , prompt_id = None , # checkout collector.log_prompt() to log your user prompts ) if user_feedback : st . write ( \"#### Raw feedback saved to Trubrics:\" ) st . write ( user_feedback ) For a full examples logging user prompts and feedback in Streamlit, see our Streamlit integration docs .","title":"Collect user prompts &amp; feedback from a Streamlit app"},{"location":"#collect-user-feedback-from-a-reactjs-app","text":"To collect user feedback from a React application, check out this example .","title":"Collect user feedback from a React.js app"},{"location":"#whats-next","text":"If you haven't already, create a free account or sign in to Trubrics . Get more technical information from our docs : Collect & analyse user prompts Collect & analyse user feedback Manage user feedback with Issues Check out our website for more information about Trubrics.","title":"What's next?"},{"location":"integrations/flask_example/","text":"Collect feedback from a Flask app The following example shows how you can integrate Trubrics feedback directly into your Flask application. We will be using Flask templates to render a UI (with HTML & CSS) that displays some different feedback types that you can collect and save to Trubrics. Install Install Trubrics & Flask to your virtual environment with pip install trubrics flask Run the example app Set your Trubrics email & password to the following environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" export TRUBRICS_COMPONENT = \"trubrics_component_name\" To directly run the application, clone the trubrics-sdk and run the following command from the root directory: flask --app examples/feedback/flask/flask_app.py --debug run Now open http://127.0.0.1:5000 to render the UI: You can now navigate to Trubrics to manage the feedback that you have collected. In this example we have built all three feedback types, but only one should be used per feedback component.","title":"Flask"},{"location":"integrations/flask_example/#collect-feedback-from-a-flask-app","text":"The following example shows how you can integrate Trubrics feedback directly into your Flask application. We will be using Flask templates to render a UI (with HTML & CSS) that displays some different feedback types that you can collect and save to Trubrics.","title":"Collect feedback from a Flask app"},{"location":"integrations/flask_example/#install","text":"Install Trubrics & Flask to your virtual environment with pip install trubrics flask","title":"Install"},{"location":"integrations/flask_example/#run-the-example-app","text":"Set your Trubrics email & password to the following environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" export TRUBRICS_COMPONENT = \"trubrics_component_name\" To directly run the application, clone the trubrics-sdk and run the following command from the root directory: flask --app examples/feedback/flask/flask_app.py --debug run Now open http://127.0.0.1:5000 to render the UI: You can now navigate to Trubrics to manage the feedback that you have collected. In this example we have built all three feedback types, but only one should be used per feedback component.","title":"Run the example app"},{"location":"integrations/langchain/","text":"TrubricsCallbackHandler integration with \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain Trubrics has an official integration with LangChain that allows developers to save user prompts directly from their LangChain LLMs or Chat models. See the integration in LangChain's documentation here.","title":"\ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"},{"location":"integrations/langchain/#trubricscallbackhandler-integration-with-langchain","text":"Trubrics has an official integration with LangChain that allows developers to save user prompts directly from their LangChain LLMs or Chat models. See the integration in LangChain's documentation here.","title":"TrubricsCallbackHandler integration with \ud83e\udd9c\ufe0f\ud83d\udd17 LangChain"},{"location":"integrations/react_js/","text":"React example with Trubrics The following example shows how you can collect feedback directly from your React.js application. We will be using React MUI components that displays thumbs up/down feedback buttons. Upon clicking, users will be able to provide extra additional feedback through a popup window. Setup To get started, clone the trubrics-sdk . Copy & rename .env_example to a .env file and add your Trubrics email, password, and a feedback component name. Use REACT_APP_TRUBRICS_COMPONENT=default if you haven't created a new feedback component yet. Run the example app npm start runs the app in the development mode from the directory examples/feedback/react_js . Open http://localhost:3000 to view it in your browser. The page will reload when you make changes. You may also see any lint errors in the console. This project was bootstrapped with Create React App . View the code The React components can be seen here .","title":"React JS"},{"location":"integrations/react_js/#react-example-with-trubrics","text":"The following example shows how you can collect feedback directly from your React.js application. We will be using React MUI components that displays thumbs up/down feedback buttons. Upon clicking, users will be able to provide extra additional feedback through a popup window.","title":"React example with Trubrics"},{"location":"integrations/react_js/#setup","text":"To get started, clone the trubrics-sdk . Copy & rename .env_example to a .env file and add your Trubrics email, password, and a feedback component name. Use REACT_APP_TRUBRICS_COMPONENT=default if you haven't created a new feedback component yet.","title":"Setup"},{"location":"integrations/react_js/#run-the-example-app","text":"npm start runs the app in the development mode from the directory examples/feedback/react_js . Open http://localhost:3000 to view it in your browser. The page will reload when you make changes. You may also see any lint errors in the console. This project was bootstrapped with Create React App .","title":"Run the example app"},{"location":"integrations/react_js/#view-the-code","text":"The React components can be seen here .","title":"View the code"},{"location":"integrations/streamlit/","text":"FeedbackCollector Streamlit Integration The FeedbackCollector takes user feedback from within an app and saves it to Trubrics. Install To get started with Streamlit , install the additional dependency: pip install \"trubrics[streamlit]\" Streamlit Example Apps Once you have created an account with Trubrics , you can try our deployed example Streamlit apps that use the integration to save feedback: LLM chat - deployed app | code : A chatbot that queries OpenAI's API and allows users to leave feedback. LLM single answer - deployed app | code : An LLM app that queries OpenAI's API and allows users to leave feedback on single text generations. The code for these apps can be viewed in the trubrics-sdk , and may be run by cloning the repo and running: LLM chat LLM single answer Tip To run this app, you are required to have your own OpenAI API key. Install openai: pip install openai Then save your OpenAI API key with OPENAI_API_KEY='your_openai_key' in st.secrets , and run: streamlit run examples/feedback/streamlit/llm_chatbot.py Tip To run this app, you are required to have your own OpenAI API key. Install openai: pip install openai Then save your OpenAI API key with OPENAI_API_KEY='your_openai_key' in st.secrets , and run: streamlit run examples/feedback/streamlit/llm_app.py Add the FeedbackCollector to your App Here is a complete example to log user prompts and feedback from a simple streamlit application: examples/streamlit/basic_app.py import streamlit as st from trubrics.integrations.streamlit import FeedbackCollector if \"logged_prompt\" not in st . session_state : st . session_state . logged_prompt = None if \"feedback_key\" not in st . session_state : st . session_state . feedback_key = 0 # 1. authenticate with trubrics collector = FeedbackCollector ( email = st . secrets . TRUBRICS_EMAIL , password = st . secrets . TRUBRICS_PASSWORD , project = \"default\" ) if st . button ( \"Refresh\" ): st . session_state . feedback_key += 1 st . session_state . logged_prompt = None st . experimental_rerun () prompt = \"Tell me a joke\" generation = \"Why did the chicken cross the road? To get to the other side.\" st . write ( f \"#### :orange[Example user prompt: { prompt } ]\" ) if st . button ( \"Generate response\" ): # 2. log a user prompt & model response st . session_state . logged_prompt = collector . log_prompt ( config_model = { \"model\" : \"gpt-3.5-turbo\" }, prompt = prompt , generation = generation , ) if st . session_state . logged_prompt : st . write ( f \"#### :blue[Example model generation: { generation } ]\" ) # 3. log some user feedback user_feedback = collector . st_feedback ( component = \"default\" , feedback_type = \"thumbs\" , open_feedback_label = \"[Optional] Provide additional feedback\" , model = st . session_state . logged_prompt . config_model . model , prompt_id = st . session_state . logged_prompt . id , key = st . session_state . feedback_key , align = \"flex-start\" , ) What's going on here? Let's break down this snippet: 1. FeedbackCollector() Tip The authentication token is cached already, but to optimise your app further, wrap the FeedbackCollector in @st.cache_data . FeedbackCollector object Parameters: Name Type Description Default project Optional [ str ] a Trubrics project name required email Optional [ str ] a Trubrics account email required password Optional [ str ] a Trubrics account password required Source code in trubrics/integrations/streamlit/collect.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , project : Optional [ str ], email : Optional [ str ], password : Optional [ str ], firebase_api_key : Optional [ str ] = None , firebase_project_id : Optional [ str ] = None , ): \"\"\" Args: project: a Trubrics project name email: a Trubrics account email password: a Trubrics account password \"\"\" if email and password and project : super () . __init__ ( email = email , password = password , project = project , firebase_api_key = firebase_api_key , firebase_project_id = firebase_project_id , ) 2. collector.log_prompt() .log_prompt() parameters Log user prompts to Trubrics. Parameters: Name Type Description Default config_model dict model configuration with fields \"model\", \"prompt_template\", \"temperature\" required prompt str user prompt to the model required generation str model generation required user_id Optional [ str ] user id None session_id Optional [ str ] session id, for example for a chatbot conversation None tags list feedback tags [] metadata dict any feedback metadata {} Source code in trubrics/platform/__init__.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def log_prompt ( self , config_model : dict , prompt : str , generation : str , user_id : Optional [ str ] = None , session_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, ) -> Optional [ Prompt ]: \"\"\" Log user prompts to Trubrics. Parameters: config_model: model configuration with fields \"model\", \"prompt_template\", \"temperature\" prompt: user prompt to the model generation: model generation user_id: user id session_id: session id, for example for a chatbot conversation tags: feedback tags metadata: any feedback metadata \"\"\" config_model = ModelConfig ( ** config_model ) prompt = Prompt ( config_model = config_model , prompt = prompt , generation = generation , user_id = user_id , session_id = session_id , tags = tags , metadata = metadata , ) auth = get_trubrics_auth_token ( self . config . firebase_api_key , self . config . email , self . config . password . get_secret_value (), rerun = expire_after_n_seconds (), ) res = save_document_to_collection ( auth , firestore_api_url = self . config . firestore_api_url , project = self . config . project , collection = \"prompts\" , document = prompt , ) if \"error\" in res : logger . error ( res [ \"error\" ]) return None else : logger . info ( \"User prompt saved to Trubrics.\" ) prompt . id = res [ \"name\" ] . split ( \"/\" )[ - 1 ] return prompt 3. collector.st_feedback() .st_feedback() parameters Collect ML model user feedback with UI components from a Streamlit app. Parameters: Name Type Description Default component str component name. Create a new component directly in Trubrics. required feedback_type str type of feedback to be collected textbox: open textbox feedback thumbs: \ud83d\udc4d / \ud83d\udc4e UI buttons faces: \ud83d\ude1e / \ud83d\ude41 / \ud83d\ude10 / \ud83d\ude42 / \ud83d\ude00 UI buttons required textbox_type str if textbox selected as feedback_type, the type of textbox to use [\"text-input\", \"text-area\"] 'text-input' model str the model used - can be a model version, a link to the saved model artifact in cloud storage, etc required prompt_id Optional [ str ] id of the prompt object None tags list a list of tags for the feedback [] metadata dict any data to save with the feedback {} user_id Optional [ str ] an optional reference to a user, for example a username if there is a login, a cookie ID, etc None key Optional [ str ] a key for the streamlit components (necessary if calling this method multiple times) None open_feedback_label Optional [ str ] label of optional text_input for \"faces\" or \"thumbs\" feedback_type None save_to_trubrics bool whether to save the feedback to Trubrics, or just to return the feedback object True disable_with_score Optional [ str ] an optional score to disable the component. Must be a \"thumbs\" emoji or a \"faces\" emoji. Can be used to pass state from one component to another. None align str where to align the feedback component [\"flex-end\", \"center\", \"flex-start\"] 'flex-end' success_fail_message bool whether to display an st.toast message on feedback submission. True Source code in trubrics/integrations/streamlit/collect.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def st_feedback ( self , component : str , feedback_type : str , model : str , textbox_type : str = \"text-input\" , prompt_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, user_id : Optional [ str ] = None , key : Optional [ str ] = None , open_feedback_label : Optional [ str ] = None , save_to_trubrics : bool = True , align : str = \"flex-end\" , disable_with_score : Optional [ str ] = None , success_fail_message : bool = True , ) -> Optional [ dict ]: \"\"\" Collect ML model user feedback with UI components from a Streamlit app. Args: component: component name. Create a new component directly in Trubrics. feedback_type: type of feedback to be collected - textbox: open textbox feedback - thumbs: \ud83d\udc4d / \ud83d\udc4e UI buttons - faces: \ud83d\ude1e / \ud83d\ude41 / \ud83d\ude10 / \ud83d\ude42 / \ud83d\ude00 UI buttons textbox_type: if textbox selected as feedback_type, the type of textbox to use [\"text-input\", \"text-area\"] model: the model used - can be a model version, a link to the saved model artifact in cloud storage, etc prompt_id: id of the prompt object tags: a list of tags for the feedback metadata: any data to save with the feedback user_id: an optional reference to a user, for example a username if there is a login, a cookie ID, etc key: a key for the streamlit components (necessary if calling this method multiple times) open_feedback_label: label of optional text_input for \"faces\" or \"thumbs\" feedback_type save_to_trubrics: whether to save the feedback to Trubrics, or just to return the feedback object disable_with_score: an optional score to disable the component. Must be a \"thumbs\" emoji or a \"faces\" emoji. Can be used to pass state from one component to another. align: where to align the feedback component [\"flex-end\", \"center\", \"flex-start\"] success_fail_message: whether to display an st.toast message on feedback submission. \"\"\" if key is None : key = feedback_type if feedback_type == \"textbox\" : text = self . st_textbox_ui ( type = textbox_type , key = key , label = open_feedback_label ) if text : user_response = { \"type\" : feedback_type , \"score\" : None , \"text\" : text } if save_to_trubrics : feedback = self . log_feedback ( component = component , user_response = user_response , model = model , prompt_id = prompt_id , metadata = metadata , tags = tags , user_id = user_id , ) if feedback is None : error_msg = \"Error in pushing feedback issue to Trubrics.\" if success_fail_message : st . error ( error_msg ) else : if success_fail_message : st . success ( \"Feedback saved to Trubrics.\" ) return self . _pydantic_to_dict ( feedback ) else : user_response = Response ( ** user_response ) feedback = Feedback ( component = component , model = model , user_response = user_response , prompt_id = prompt_id , user_id = user_id , tags = tags , metadata = metadata , ) return self . _pydantic_to_dict ( feedback ) elif feedback_type in ( \"thumbs\" , \"faces\" ): def _log_feedback_trubrics ( user_response , ** kwargs ): feedback = self . log_feedback ( user_response = user_response , ** kwargs ) if success_fail_message : if feedback : st . toast ( \"Feedback saved to [Trubrics](https://trubrics.streamlit.app/).\" , icon = \"\u2705\" ) return self . _pydantic_to_dict ( feedback ) else : st . toast ( \"Error in saving feedback to [Trubrics](https://trubrics.streamlit.app/).\" , icon = \"\u274c\" ) user_response = streamlit_feedback ( feedback_type = feedback_type , optional_text_label = open_feedback_label , disable_with_score = disable_with_score , on_submit = _log_feedback_trubrics if save_to_trubrics else None , kwargs = { \"component\" : component , \"model\" : model , \"prompt_id\" : prompt_id , \"metadata\" : metadata , \"tags\" : tags , \"user_id\" : user_id , }, align = align , key = key , ) if save_to_trubrics is False and user_response : user_response = Response ( ** user_response ) feedback = Feedback ( component = component , model = model , user_response = user_response , prompt_id = prompt_id , user_id = user_id , tags = tags , metadata = metadata , ) return self . _pydantic_to_dict ( feedback ) return user_response else : raise ValueError ( \"feedback_type must be one of ['textbox', 'faces', 'thumbs'].\" ) return None","title":"Streamlit"},{"location":"integrations/streamlit/#feedbackcollector-streamlit-integration","text":"The FeedbackCollector takes user feedback from within an app and saves it to Trubrics.","title":"FeedbackCollector Streamlit Integration"},{"location":"integrations/streamlit/#install","text":"To get started with Streamlit , install the additional dependency: pip install \"trubrics[streamlit]\"","title":"Install"},{"location":"integrations/streamlit/#streamlit-example-apps","text":"Once you have created an account with Trubrics , you can try our deployed example Streamlit apps that use the integration to save feedback: LLM chat - deployed app | code : A chatbot that queries OpenAI's API and allows users to leave feedback. LLM single answer - deployed app | code : An LLM app that queries OpenAI's API and allows users to leave feedback on single text generations. The code for these apps can be viewed in the trubrics-sdk , and may be run by cloning the repo and running: LLM chat LLM single answer Tip To run this app, you are required to have your own OpenAI API key. Install openai: pip install openai Then save your OpenAI API key with OPENAI_API_KEY='your_openai_key' in st.secrets , and run: streamlit run examples/feedback/streamlit/llm_chatbot.py Tip To run this app, you are required to have your own OpenAI API key. Install openai: pip install openai Then save your OpenAI API key with OPENAI_API_KEY='your_openai_key' in st.secrets , and run: streamlit run examples/feedback/streamlit/llm_app.py","title":"Streamlit Example Apps"},{"location":"integrations/streamlit/#add-the-feedbackcollector-to-your-app","text":"Here is a complete example to log user prompts and feedback from a simple streamlit application: examples/streamlit/basic_app.py import streamlit as st from trubrics.integrations.streamlit import FeedbackCollector if \"logged_prompt\" not in st . session_state : st . session_state . logged_prompt = None if \"feedback_key\" not in st . session_state : st . session_state . feedback_key = 0 # 1. authenticate with trubrics collector = FeedbackCollector ( email = st . secrets . TRUBRICS_EMAIL , password = st . secrets . TRUBRICS_PASSWORD , project = \"default\" ) if st . button ( \"Refresh\" ): st . session_state . feedback_key += 1 st . session_state . logged_prompt = None st . experimental_rerun () prompt = \"Tell me a joke\" generation = \"Why did the chicken cross the road? To get to the other side.\" st . write ( f \"#### :orange[Example user prompt: { prompt } ]\" ) if st . button ( \"Generate response\" ): # 2. log a user prompt & model response st . session_state . logged_prompt = collector . log_prompt ( config_model = { \"model\" : \"gpt-3.5-turbo\" }, prompt = prompt , generation = generation , ) if st . session_state . logged_prompt : st . write ( f \"#### :blue[Example model generation: { generation } ]\" ) # 3. log some user feedback user_feedback = collector . st_feedback ( component = \"default\" , feedback_type = \"thumbs\" , open_feedback_label = \"[Optional] Provide additional feedback\" , model = st . session_state . logged_prompt . config_model . model , prompt_id = st . session_state . logged_prompt . id , key = st . session_state . feedback_key , align = \"flex-start\" , ) What's going on here? Let's break down this snippet:","title":"Add the FeedbackCollector to your App"},{"location":"integrations/streamlit/#1-feedbackcollector","text":"Tip The authentication token is cached already, but to optimise your app further, wrap the FeedbackCollector in @st.cache_data . FeedbackCollector object Parameters: Name Type Description Default project Optional [ str ] a Trubrics project name required email Optional [ str ] a Trubrics account email required password Optional [ str ] a Trubrics account password required Source code in trubrics/integrations/streamlit/collect.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self , project : Optional [ str ], email : Optional [ str ], password : Optional [ str ], firebase_api_key : Optional [ str ] = None , firebase_project_id : Optional [ str ] = None , ): \"\"\" Args: project: a Trubrics project name email: a Trubrics account email password: a Trubrics account password \"\"\" if email and password and project : super () . __init__ ( email = email , password = password , project = project , firebase_api_key = firebase_api_key , firebase_project_id = firebase_project_id , )","title":"1. FeedbackCollector()"},{"location":"integrations/streamlit/#2-collectorlog_prompt","text":".log_prompt() parameters Log user prompts to Trubrics. Parameters: Name Type Description Default config_model dict model configuration with fields \"model\", \"prompt_template\", \"temperature\" required prompt str user prompt to the model required generation str model generation required user_id Optional [ str ] user id None session_id Optional [ str ] session id, for example for a chatbot conversation None tags list feedback tags [] metadata dict any feedback metadata {} Source code in trubrics/platform/__init__.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def log_prompt ( self , config_model : dict , prompt : str , generation : str , user_id : Optional [ str ] = None , session_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, ) -> Optional [ Prompt ]: \"\"\" Log user prompts to Trubrics. Parameters: config_model: model configuration with fields \"model\", \"prompt_template\", \"temperature\" prompt: user prompt to the model generation: model generation user_id: user id session_id: session id, for example for a chatbot conversation tags: feedback tags metadata: any feedback metadata \"\"\" config_model = ModelConfig ( ** config_model ) prompt = Prompt ( config_model = config_model , prompt = prompt , generation = generation , user_id = user_id , session_id = session_id , tags = tags , metadata = metadata , ) auth = get_trubrics_auth_token ( self . config . firebase_api_key , self . config . email , self . config . password . get_secret_value (), rerun = expire_after_n_seconds (), ) res = save_document_to_collection ( auth , firestore_api_url = self . config . firestore_api_url , project = self . config . project , collection = \"prompts\" , document = prompt , ) if \"error\" in res : logger . error ( res [ \"error\" ]) return None else : logger . info ( \"User prompt saved to Trubrics.\" ) prompt . id = res [ \"name\" ] . split ( \"/\" )[ - 1 ] return prompt","title":"2. collector.log_prompt()"},{"location":"integrations/streamlit/#3-collectorst_feedback","text":".st_feedback() parameters Collect ML model user feedback with UI components from a Streamlit app. Parameters: Name Type Description Default component str component name. Create a new component directly in Trubrics. required feedback_type str type of feedback to be collected textbox: open textbox feedback thumbs: \ud83d\udc4d / \ud83d\udc4e UI buttons faces: \ud83d\ude1e / \ud83d\ude41 / \ud83d\ude10 / \ud83d\ude42 / \ud83d\ude00 UI buttons required textbox_type str if textbox selected as feedback_type, the type of textbox to use [\"text-input\", \"text-area\"] 'text-input' model str the model used - can be a model version, a link to the saved model artifact in cloud storage, etc required prompt_id Optional [ str ] id of the prompt object None tags list a list of tags for the feedback [] metadata dict any data to save with the feedback {} user_id Optional [ str ] an optional reference to a user, for example a username if there is a login, a cookie ID, etc None key Optional [ str ] a key for the streamlit components (necessary if calling this method multiple times) None open_feedback_label Optional [ str ] label of optional text_input for \"faces\" or \"thumbs\" feedback_type None save_to_trubrics bool whether to save the feedback to Trubrics, or just to return the feedback object True disable_with_score Optional [ str ] an optional score to disable the component. Must be a \"thumbs\" emoji or a \"faces\" emoji. Can be used to pass state from one component to another. None align str where to align the feedback component [\"flex-end\", \"center\", \"flex-start\"] 'flex-end' success_fail_message bool whether to display an st.toast message on feedback submission. True Source code in trubrics/integrations/streamlit/collect.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def st_feedback ( self , component : str , feedback_type : str , model : str , textbox_type : str = \"text-input\" , prompt_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, user_id : Optional [ str ] = None , key : Optional [ str ] = None , open_feedback_label : Optional [ str ] = None , save_to_trubrics : bool = True , align : str = \"flex-end\" , disable_with_score : Optional [ str ] = None , success_fail_message : bool = True , ) -> Optional [ dict ]: \"\"\" Collect ML model user feedback with UI components from a Streamlit app. Args: component: component name. Create a new component directly in Trubrics. feedback_type: type of feedback to be collected - textbox: open textbox feedback - thumbs: \ud83d\udc4d / \ud83d\udc4e UI buttons - faces: \ud83d\ude1e / \ud83d\ude41 / \ud83d\ude10 / \ud83d\ude42 / \ud83d\ude00 UI buttons textbox_type: if textbox selected as feedback_type, the type of textbox to use [\"text-input\", \"text-area\"] model: the model used - can be a model version, a link to the saved model artifact in cloud storage, etc prompt_id: id of the prompt object tags: a list of tags for the feedback metadata: any data to save with the feedback user_id: an optional reference to a user, for example a username if there is a login, a cookie ID, etc key: a key for the streamlit components (necessary if calling this method multiple times) open_feedback_label: label of optional text_input for \"faces\" or \"thumbs\" feedback_type save_to_trubrics: whether to save the feedback to Trubrics, or just to return the feedback object disable_with_score: an optional score to disable the component. Must be a \"thumbs\" emoji or a \"faces\" emoji. Can be used to pass state from one component to another. align: where to align the feedback component [\"flex-end\", \"center\", \"flex-start\"] success_fail_message: whether to display an st.toast message on feedback submission. \"\"\" if key is None : key = feedback_type if feedback_type == \"textbox\" : text = self . st_textbox_ui ( type = textbox_type , key = key , label = open_feedback_label ) if text : user_response = { \"type\" : feedback_type , \"score\" : None , \"text\" : text } if save_to_trubrics : feedback = self . log_feedback ( component = component , user_response = user_response , model = model , prompt_id = prompt_id , metadata = metadata , tags = tags , user_id = user_id , ) if feedback is None : error_msg = \"Error in pushing feedback issue to Trubrics.\" if success_fail_message : st . error ( error_msg ) else : if success_fail_message : st . success ( \"Feedback saved to Trubrics.\" ) return self . _pydantic_to_dict ( feedback ) else : user_response = Response ( ** user_response ) feedback = Feedback ( component = component , model = model , user_response = user_response , prompt_id = prompt_id , user_id = user_id , tags = tags , metadata = metadata , ) return self . _pydantic_to_dict ( feedback ) elif feedback_type in ( \"thumbs\" , \"faces\" ): def _log_feedback_trubrics ( user_response , ** kwargs ): feedback = self . log_feedback ( user_response = user_response , ** kwargs ) if success_fail_message : if feedback : st . toast ( \"Feedback saved to [Trubrics](https://trubrics.streamlit.app/).\" , icon = \"\u2705\" ) return self . _pydantic_to_dict ( feedback ) else : st . toast ( \"Error in saving feedback to [Trubrics](https://trubrics.streamlit.app/).\" , icon = \"\u274c\" ) user_response = streamlit_feedback ( feedback_type = feedback_type , optional_text_label = open_feedback_label , disable_with_score = disable_with_score , on_submit = _log_feedback_trubrics if save_to_trubrics else None , kwargs = { \"component\" : component , \"model\" : model , \"prompt_id\" : prompt_id , \"metadata\" : metadata , \"tags\" : tags , \"user_id\" : user_id , }, align = align , key = key , ) if save_to_trubrics is False and user_response : user_response = Response ( ** user_response ) feedback = Feedback ( component = component , model = model , user_response = user_response , prompt_id = prompt_id , user_id = user_id , tags = tags , metadata = metadata , ) return self . _pydantic_to_dict ( feedback ) return user_response else : raise ValueError ( \"feedback_type must be one of ['textbox', 'faces', 'thumbs'].\" ) return None","title":"3. collector.st_feedback()"},{"location":"platform/account_setup/","text":"Create an account To initially start using Trubrics , navigate to the Create an account tab. If you have forgotten your password, enter your email on the Forgot password? tab to send a password reset email. Team access Contact us to gain access for multiple users of an organisation. Projects All data in Trubrics is organised into Projects. It is up to you how you group your projects, but we suggest you create different projects per AI use case. Upon account creation, a default project is created. All user prompts are stored directly within the project, i.e. there is a single table for user prompts per project . Prompts can be filtered by model config, prompt template, tags, etc. Feedback components Within a project, you may also create feedback components to organise user feedback. Each component collects a unique type of feedback. Multiple feedback components can be implemented for the same AI use case. Create a feedback component At the top of the Feedback page in Trubrics , you can create a feedback component. To help you determine what type of feedback to use, there is a visual preview of the UI component, along with the generated code snippet for the component to include into your AI application. A default component is created in the default project upon account creation, allowing you to directly start saving prompts and \ud83d\udc4d / \ud83d\udc4e user feedback.","title":"Account Setup"},{"location":"platform/account_setup/#create-an-account","text":"To initially start using Trubrics , navigate to the Create an account tab. If you have forgotten your password, enter your email on the Forgot password? tab to send a password reset email. Team access Contact us to gain access for multiple users of an organisation.","title":"Create an account"},{"location":"platform/account_setup/#projects","text":"All data in Trubrics is organised into Projects. It is up to you how you group your projects, but we suggest you create different projects per AI use case. Upon account creation, a default project is created. All user prompts are stored directly within the project, i.e. there is a single table for user prompts per project . Prompts can be filtered by model config, prompt template, tags, etc.","title":"Projects"},{"location":"platform/account_setup/#feedback-components","text":"Within a project, you may also create feedback components to organise user feedback. Each component collects a unique type of feedback. Multiple feedback components can be implemented for the same AI use case.","title":"Feedback components"},{"location":"platform/account_setup/#create-a-feedback-component","text":"At the top of the Feedback page in Trubrics , you can create a feedback component. To help you determine what type of feedback to use, there is a visual preview of the UI component, along with the generated code snippet for the component to include into your AI application. A default component is created in the default project upon account creation, allowing you to directly start saving prompts and \ud83d\udc4d / \ud83d\udc4e user feedback.","title":"Create a feedback component"},{"location":"platform/issues/","text":"\u26a0\ufe0f Issues Issues allow AI teams to group user comments into an issue. Similar to how GitHub uses issues to track all features requests / bugs for a code repository, Trubrics applies the same philosophy to AI models. Create an issue Creating an issue requires a user to select at least one comment in the Comments tab of \ud83e\ude84 Insights, and to provide a title and description. Open / close an issue To open or close an issue, a single issue must be selected. For more context on the issue, users can search for the linked comment IDs in the Comments tab.","title":"Issues"},{"location":"platform/issues/#issues","text":"Issues allow AI teams to group user comments into an issue. Similar to how GitHub uses issues to track all features requests / bugs for a code repository, Trubrics applies the same philosophy to AI models.","title":"\u26a0\ufe0f Issues"},{"location":"platform/issues/#create-an-issue","text":"Creating an issue requires a user to select at least one comment in the Comments tab of \ud83e\ude84 Insights, and to provide a title and description.","title":"Create an issue"},{"location":"platform/issues/#open-close-an-issue","text":"To open or close an issue, a single issue must be selected. For more context on the issue, users can search for the linked comment IDs in the Comments tab.","title":"Open / close an issue"},{"location":"platform/user_feedback/","text":"Saving feedback to Trubrics Upon creating a feedback component in Trubrics , a code snippet is generated for users to incorporate into their apps. There are several ways to save feedback: 1. With the Python SDK Install Trubrics with: pip install trubrics Set Trubrics email and password as environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" and push some feedback to the default feedback component: import os from trubrics import Trubrics trubrics = Trubrics ( project = \"default\" , email = os . environ [ \"TRUBRICS_EMAIL\" ], password = os . environ [ \"TRUBRICS_PASSWORD\" ], ) user_feedback = trubrics . log_feedback ( component = \"default\" , model = \"gpt-3.5-turbo\" , prompt_id = None , # see `Prompts` to store user prompts and model generations user_response = { \"type\" : \"thumbs\" , \"score\" : \"\ud83d\udc4e\" , \"text\" : \"Not a very funny joke...\" , } ) trubrics.log_feedback() arguments Log user feedback to Trubrics. Parameters: Name Type Description Default component str feedback component name created in Trubrics required model str model name required user_response dict a user response dict that must contain these fields {\"type\": \"\", \"score\": \"\", \"text\": None} required prompt_id Optional [ str ] an optional prompt_id for tracing feedback on a specific prompt / model generation None user_id Optional [ str ] a user_id None tags list feedback tags [] metadata dict any feedback metadata {} Source code in trubrics/platform/__init__.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def log_feedback ( self , component : str , model : str , user_response : dict , prompt_id : Optional [ str ] = None , user_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, ) -> Optional [ Feedback ]: \"\"\" Log user feedback to Trubrics. Parameters: component: feedback component name created in Trubrics model: model name user_response: a user response dict that must contain these fields {\"type\": \"\", \"score\": \"\", \"text\": None} prompt_id: an optional prompt_id for tracing feedback on a specific prompt / model generation user_id: a user_id tags: feedback tags metadata: any feedback metadata \"\"\" user_response = Response ( ** user_response ) feedback = Feedback ( component = component , model = model , user_response = user_response , prompt_id = prompt_id , user_id = user_id , tags = tags , metadata = metadata , ) auth = get_trubrics_auth_token ( self . config . firebase_api_key , self . config . email , self . config . password . get_secret_value (), rerun = expire_after_n_seconds (), ) components = list_components_in_organisation ( firestore_api_url = self . config . firestore_api_url , auth = auth , project = self . config . project ) if feedback . component not in components : raise ValueError ( f \"Component ' { feedback . component } ' not found. Please select one of: { components } .\" ) res = save_document_to_collection ( auth , firestore_api_url = self . config . firestore_api_url , project = self . config . project , collection = f \"feedback/ { feedback . component } /responses\" , document = feedback , ) if \"error\" in res : logger . error ( res [ \"error\" ]) return None else : logger . info ( \"User feedback saved to Trubrics.\" ) return feedback 2. With Streamlit Trubrics has an out-of-the-box integration with Streamlit : pip install \"trubrics[streamlit]\" import streamlit as st from trubrics.integrations.streamlit import FeedbackCollector collector = FeedbackCollector ( project = \"default\" , email = st . secrets . TRUBRICS_EMAIL , password = st . secrets . TRUBRICS_PASSWORD , ) collector . st_feedback ( component = \"default\" , feedback_type = \"thumbs\" , open_feedback_label = \"[Optional] Provide additional feedback\" , model = \"gpt-3.5-turbo\" , prompt_id = None , # see `Prompts` to log prompts and model generations ) Take a look at our demo LLM app for an example. 3. With Flask Here is an example of how the python SDK can be used with a Flask app. 4. With React Here is an example showing how to collect feedback from a React app. Types of feedback Each feedback response in a component must be of a particular type, as seen in the user_response field of the Feedback data object. Feedback object There are three out-of-the-box types of feedback: thumbs feedback (\ud83d\udc4d, \ud83d\udc4e), with an optional open text box faces feedback (\ud83d\ude1e, \ud83d\ude41, \ud83d\ude10, \ud83d\ude42, \ud83d\ude00), with an optional open text box textbox feedback, an open text box for purely qualitative feedback To save custom feedback with multiple fields, such as collecting survey responses, users can make use of the Feedback metadata field. Analyse quantitative user feedback in Trubrics Various filters allow AI teams to: Aggregate responses by a frequency (hourly, daily, weekly, monthly) View all responses for a given score, model or user Compare responses for all scores, models or users Tip All quantitative analysis is viewed per feedback component. Each feedback component should have a unique set of scores (i.e a unique type ) for analysis to be correctly computed. Review user comments User comments are collected in the text field of user_response . All comments are listed in the Comments tab, and may be grouped together to create an issue . Export all raw data Export a raw json file of all responses allows AI teams to conduct their own analysis. Use the `\ud83d\udce5 Download all` button for a full export to json.","title":"User Feedback"},{"location":"platform/user_feedback/#saving-feedback-to-trubrics","text":"Upon creating a feedback component in Trubrics , a code snippet is generated for users to incorporate into their apps. There are several ways to save feedback:","title":"Saving feedback to Trubrics"},{"location":"platform/user_feedback/#1-with-the-python-sdk","text":"Install Trubrics with: pip install trubrics Set Trubrics email and password as environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" and push some feedback to the default feedback component: import os from trubrics import Trubrics trubrics = Trubrics ( project = \"default\" , email = os . environ [ \"TRUBRICS_EMAIL\" ], password = os . environ [ \"TRUBRICS_PASSWORD\" ], ) user_feedback = trubrics . log_feedback ( component = \"default\" , model = \"gpt-3.5-turbo\" , prompt_id = None , # see `Prompts` to store user prompts and model generations user_response = { \"type\" : \"thumbs\" , \"score\" : \"\ud83d\udc4e\" , \"text\" : \"Not a very funny joke...\" , } ) trubrics.log_feedback() arguments Log user feedback to Trubrics. Parameters: Name Type Description Default component str feedback component name created in Trubrics required model str model name required user_response dict a user response dict that must contain these fields {\"type\": \"\", \"score\": \"\", \"text\": None} required prompt_id Optional [ str ] an optional prompt_id for tracing feedback on a specific prompt / model generation None user_id Optional [ str ] a user_id None tags list feedback tags [] metadata dict any feedback metadata {} Source code in trubrics/platform/__init__.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def log_feedback ( self , component : str , model : str , user_response : dict , prompt_id : Optional [ str ] = None , user_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, ) -> Optional [ Feedback ]: \"\"\" Log user feedback to Trubrics. Parameters: component: feedback component name created in Trubrics model: model name user_response: a user response dict that must contain these fields {\"type\": \"\", \"score\": \"\", \"text\": None} prompt_id: an optional prompt_id for tracing feedback on a specific prompt / model generation user_id: a user_id tags: feedback tags metadata: any feedback metadata \"\"\" user_response = Response ( ** user_response ) feedback = Feedback ( component = component , model = model , user_response = user_response , prompt_id = prompt_id , user_id = user_id , tags = tags , metadata = metadata , ) auth = get_trubrics_auth_token ( self . config . firebase_api_key , self . config . email , self . config . password . get_secret_value (), rerun = expire_after_n_seconds (), ) components = list_components_in_organisation ( firestore_api_url = self . config . firestore_api_url , auth = auth , project = self . config . project ) if feedback . component not in components : raise ValueError ( f \"Component ' { feedback . component } ' not found. Please select one of: { components } .\" ) res = save_document_to_collection ( auth , firestore_api_url = self . config . firestore_api_url , project = self . config . project , collection = f \"feedback/ { feedback . component } /responses\" , document = feedback , ) if \"error\" in res : logger . error ( res [ \"error\" ]) return None else : logger . info ( \"User feedback saved to Trubrics.\" ) return feedback","title":"1. With the Python SDK"},{"location":"platform/user_feedback/#2-with-streamlit","text":"Trubrics has an out-of-the-box integration with Streamlit : pip install \"trubrics[streamlit]\" import streamlit as st from trubrics.integrations.streamlit import FeedbackCollector collector = FeedbackCollector ( project = \"default\" , email = st . secrets . TRUBRICS_EMAIL , password = st . secrets . TRUBRICS_PASSWORD , ) collector . st_feedback ( component = \"default\" , feedback_type = \"thumbs\" , open_feedback_label = \"[Optional] Provide additional feedback\" , model = \"gpt-3.5-turbo\" , prompt_id = None , # see `Prompts` to log prompts and model generations ) Take a look at our demo LLM app for an example.","title":"2. With Streamlit"},{"location":"platform/user_feedback/#3-with-flask","text":"Here is an example of how the python SDK can be used with a Flask app.","title":"3. With Flask"},{"location":"platform/user_feedback/#4-with-react","text":"Here is an example showing how to collect feedback from a React app.","title":"4. With React"},{"location":"platform/user_feedback/#types-of-feedback","text":"Each feedback response in a component must be of a particular type, as seen in the user_response field of the Feedback data object. Feedback object There are three out-of-the-box types of feedback: thumbs feedback (\ud83d\udc4d, \ud83d\udc4e), with an optional open text box faces feedback (\ud83d\ude1e, \ud83d\ude41, \ud83d\ude10, \ud83d\ude42, \ud83d\ude00), with an optional open text box textbox feedback, an open text box for purely qualitative feedback To save custom feedback with multiple fields, such as collecting survey responses, users can make use of the Feedback metadata field.","title":"Types of feedback"},{"location":"platform/user_feedback/#analyse-quantitative-user-feedback-in-trubrics","text":"Various filters allow AI teams to: Aggregate responses by a frequency (hourly, daily, weekly, monthly) View all responses for a given score, model or user Compare responses for all scores, models or users Tip All quantitative analysis is viewed per feedback component. Each feedback component should have a unique set of scores (i.e a unique type ) for analysis to be correctly computed.","title":"Analyse quantitative user feedback in Trubrics"},{"location":"platform/user_feedback/#review-user-comments","text":"User comments are collected in the text field of user_response . All comments are listed in the Comments tab, and may be grouped together to create an issue .","title":"Review user comments"},{"location":"platform/user_feedback/#export-all-raw-data","text":"Export a raw json file of all responses allows AI teams to conduct their own analysis. Use the `\ud83d\udce5 Download all` button for a full export to json.","title":"Export all raw data"},{"location":"platform/user_prompts/","text":"Saving prompts to Trubrics Analysing user prompts is essential to building AI models that aligns with your users. Upon creating an account with Trubrics , you can start logging prompts & model generations to the default project. Create different projects to organise your prompts (we recommend one project per use case). Install Trubrics with: pip install trubrics Set Trubrics email and password as environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" and push some user prompts to the default project: import os from trubrics import Trubrics trubrics = Trubrics ( project = \"default\" , email = os . environ [ \"TRUBRICS_EMAIL\" ], password = os . environ [ \"TRUBRICS_PASSWORD\" ], ) prompt = trubrics . log_prompt ( config_model = {{ \"model\" : \"gpt-3.5-turbo\" , \"prompt_template\" : \"Tell me a joke about {{animal}}\" , \"temperature\" : 0.7 , }}, prompt = \"Tell me a joke about sharks\" , generation = \"Why did the shark cross the ocean? To get to the other side.\" ) trubrics.log_prompt() arguments Log user prompts to Trubrics. Parameters: Name Type Description Default config_model dict model configuration with fields \"model\", \"prompt_template\", \"temperature\" required prompt str user prompt to the model required generation str model generation required user_id Optional [ str ] user id None session_id Optional [ str ] session id, for example for a chatbot conversation None tags list feedback tags [] metadata dict any feedback metadata {} Source code in trubrics/platform/__init__.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def log_prompt ( self , config_model : dict , prompt : str , generation : str , user_id : Optional [ str ] = None , session_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, ) -> Optional [ Prompt ]: \"\"\" Log user prompts to Trubrics. Parameters: config_model: model configuration with fields \"model\", \"prompt_template\", \"temperature\" prompt: user prompt to the model generation: model generation user_id: user id session_id: session id, for example for a chatbot conversation tags: feedback tags metadata: any feedback metadata \"\"\" config_model = ModelConfig ( ** config_model ) prompt = Prompt ( config_model = config_model , prompt = prompt , generation = generation , user_id = user_id , session_id = session_id , tags = tags , metadata = metadata , ) auth = get_trubrics_auth_token ( self . config . firebase_api_key , self . config . email , self . config . password . get_secret_value (), rerun = expire_after_n_seconds (), ) res = save_document_to_collection ( auth , firestore_api_url = self . config . firestore_api_url , project = self . config . project , collection = \"prompts\" , document = prompt , ) if \"error\" in res : logger . error ( res [ \"error\" ]) return None else : logger . info ( \"User prompt saved to Trubrics.\" ) prompt . id = res [ \"name\" ] . split ( \"/\" )[ - 1 ] return prompt Saving prompts from Streamlit apps The FeedbackCollector Streamlit integration inherits from the Trubrics object, meaning that you can log prompts in the same way directly from the FeedbackCollector . For more information on this, see the Streamlit integration docs. Analyse prompts in Trubrics Various filters allow AI teams to explore user prompts in Trubrics, and export them to csv. [Beta] Ask an LLM about your prompts Try asking an LLM a question about the user prompts in your dataset. For example, \"What prompts ask for shark jokes?\". This feature is in beta, so please give us feedback on the model generations!","title":"User Prompts"},{"location":"platform/user_prompts/#saving-prompts-to-trubrics","text":"Analysing user prompts is essential to building AI models that aligns with your users. Upon creating an account with Trubrics , you can start logging prompts & model generations to the default project. Create different projects to organise your prompts (we recommend one project per use case). Install Trubrics with: pip install trubrics Set Trubrics email and password as environment variables: export TRUBRICS_EMAIL = \"trubrics_email\" export TRUBRICS_PASSWORD = \"trubrics_password\" and push some user prompts to the default project: import os from trubrics import Trubrics trubrics = Trubrics ( project = \"default\" , email = os . environ [ \"TRUBRICS_EMAIL\" ], password = os . environ [ \"TRUBRICS_PASSWORD\" ], ) prompt = trubrics . log_prompt ( config_model = {{ \"model\" : \"gpt-3.5-turbo\" , \"prompt_template\" : \"Tell me a joke about {{animal}}\" , \"temperature\" : 0.7 , }}, prompt = \"Tell me a joke about sharks\" , generation = \"Why did the shark cross the ocean? To get to the other side.\" ) trubrics.log_prompt() arguments Log user prompts to Trubrics. Parameters: Name Type Description Default config_model dict model configuration with fields \"model\", \"prompt_template\", \"temperature\" required prompt str user prompt to the model required generation str model generation required user_id Optional [ str ] user id None session_id Optional [ str ] session id, for example for a chatbot conversation None tags list feedback tags [] metadata dict any feedback metadata {} Source code in trubrics/platform/__init__.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def log_prompt ( self , config_model : dict , prompt : str , generation : str , user_id : Optional [ str ] = None , session_id : Optional [ str ] = None , tags : list = [], metadata : dict = {}, ) -> Optional [ Prompt ]: \"\"\" Log user prompts to Trubrics. Parameters: config_model: model configuration with fields \"model\", \"prompt_template\", \"temperature\" prompt: user prompt to the model generation: model generation user_id: user id session_id: session id, for example for a chatbot conversation tags: feedback tags metadata: any feedback metadata \"\"\" config_model = ModelConfig ( ** config_model ) prompt = Prompt ( config_model = config_model , prompt = prompt , generation = generation , user_id = user_id , session_id = session_id , tags = tags , metadata = metadata , ) auth = get_trubrics_auth_token ( self . config . firebase_api_key , self . config . email , self . config . password . get_secret_value (), rerun = expire_after_n_seconds (), ) res = save_document_to_collection ( auth , firestore_api_url = self . config . firestore_api_url , project = self . config . project , collection = \"prompts\" , document = prompt , ) if \"error\" in res : logger . error ( res [ \"error\" ]) return None else : logger . info ( \"User prompt saved to Trubrics.\" ) prompt . id = res [ \"name\" ] . split ( \"/\" )[ - 1 ] return prompt","title":"Saving prompts to Trubrics"},{"location":"platform/user_prompts/#saving-prompts-from-streamlit-apps","text":"The FeedbackCollector Streamlit integration inherits from the Trubrics object, meaning that you can log prompts in the same way directly from the FeedbackCollector . For more information on this, see the Streamlit integration docs.","title":"Saving prompts from Streamlit apps"},{"location":"platform/user_prompts/#analyse-prompts-in-trubrics","text":"Various filters allow AI teams to explore user prompts in Trubrics, and export them to csv. [Beta] Ask an LLM about your prompts Try asking an LLM a question about the user prompts in your dataset. For example, \"What prompts ask for shark jokes?\". This feature is in beta, so please give us feedback on the model generations!","title":"Analyse prompts in Trubrics"}]}